{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rhqo/lora/blob/master/Korean_LoRA_Finetuning_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# ğŸ‡°ğŸ‡· í•œêµ­ì–´ LoRA Fine-tuning with TinyLlama\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/your-repo/Korean_LoRA_Finetuning_Colab.ipynb)\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“Œ ì´ ë…¸íŠ¸ë¶ì˜ ëª©ì \n",
        "\n",
        "ì˜ì–´ ê¸°ë°˜ TinyLlama ëª¨ë¸ì„ **LoRA(Low-Rank Adaptation)**ë¥¼ ì‚¬ìš©í•˜ì—¬ í•œêµ­ì–´ ë°ì´í„°ë¡œ fine-tuningí•©ë‹ˆë‹¤.\n",
        "\n",
        "### ğŸ¯ í•™ìŠµ íš¨ê³¼\n",
        "- **Before**: í•œêµ­ì–´ ì§ˆë¬¸ì— ì˜ì–´ë¡œ ë‹µë³€í•˜ê±°ë‚˜ ì´ìƒí•œ ë‹µë³€\n",
        "- **After**: ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ì •í™•í•œ ë‹µë³€!\n",
        "\n",
        "### â±ï¸ ì˜ˆìƒ ì†Œìš” ì‹œê°„\n",
        "- ì„¤ì¹˜: ~3ë¶„\n",
        "- í•™ìŠµ: ~15-20ë¶„ (T4 GPU ê¸°ì¤€)\n",
        "\n",
        "---\n",
        "\n",
        "## âš ï¸ ì‹œì‘í•˜ê¸° ì „ì—\n",
        "\n",
        "### 1. GPU ì„¤ì • í™•ì¸\n",
        "ìƒë‹¨ ë©”ë‰´: **ëŸ°íƒ€ì„(Runtime) > ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½(Change runtime type) > í•˜ë“œì›¨ì–´ ê°€ì†ê¸°(Hardware accelerator) > GPU > T4**\n",
        "\n",
        "### 2. ë…¸íŠ¸ë¶ ì‹¤í–‰ ë°©ë²•\n",
        "- ê° ì…€ì„ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰ (Shift + Enter)\n",
        "- ë˜ëŠ” ìƒë‹¨ ë©”ë‰´: **ëŸ°íƒ€ì„ > ëª¨ë‘ ì‹¤í–‰**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## 1ï¸âƒ£ í™˜ê²½ ì„¤ì •"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "check_gpu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa3864fb-8828-46df-d557-1a1bc67e7c8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Oct 16 16:07:53 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# GPU í™•ì¸\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_packages",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca4af917-1ad0-424a-a420-9d91a946c433"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¦ Installing packages...\n",
            "âœ… Installation complete!\n"
          ]
        }
      ],
      "source": [
        "# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ (~3ë¶„ ì†Œìš”)\n",
        "print(\"ğŸ“¦ Installing packages...\")\n",
        "\n",
        "!pip install -q -U \\\n",
        "    torch>=2.2.0 \\\n",
        "    transformers==4.41.2 \\\n",
        "    datasets==2.18.0 \\\n",
        "    accelerate==0.31.0 \\\n",
        "    peft==0.11.1 \\\n",
        "    trl==0.9.4 \\\n",
        "    bitsandbytes==0.43.1 \\\n",
        "    sentencepiece==0.2.0 \\\n",
        "    protobuf\n",
        "\n",
        "print(\"âœ… Installation complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "outputId": "dad554b4-f430-42ce-9e9a-e3e4339fbe07"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… PyTorch version: 2.9.0+cu128\n",
            "âœ… CUDA available: True\n",
            "âœ… GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    BitsAndBytesConfig,\n",
        "    pipeline\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, AutoPeftModelForCausalLM, prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "print(f\"âœ… PyTorch version: {torch.__version__}\")\n",
        "print(f\"âœ… CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"âœ… GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "config"
      },
      "source": [
        "## 2ï¸âƒ£ ì„¤ì • ë° í•˜ì´í¼íŒŒë¼ë¯¸í„°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyperparameters",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8b5e3b3-2c8a-454c-d000-31e8e033ebd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“‹ Configuration:\n",
            "   Model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
            "   Dataset: nlpai-lab/kullm-v2 (3000 samples)\n",
            "   LoRA Rank: 16\n",
            "   Epochs: 3\n",
            "   Effective Batch Size: 16\n",
            "   4-bit Quantization: True\n"
          ]
        }
      ],
      "source": [
        "# ====== ì´ ë¶€ë¶„ì„ ìˆ˜ì •í•˜ì—¬ ì‹¤í—˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ ======\n",
        "\n",
        "# ëª¨ë¸ ì„¤ì •\n",
        "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "OUTPUT_DIR = \"./korean-tinyllama-lora\"\n",
        "\n",
        "# ë°ì´í„° ì„¤ì •\n",
        "DATASET_NAME = \"nlpai-lab/kullm-v2\"\n",
        "NUM_SAMPLES = 3000\n",
        "\n",
        "# LoRA í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
        "LORA_R = 16\n",
        "LORA_ALPHA = 32\n",
        "LORA_DROPOUT = 0.05\n",
        "TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
        "\n",
        "# í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
        "NUM_EPOCHS = 3\n",
        "BATCH_SIZE = 4\n",
        "GRADIENT_ACCUMULATION_STEPS = 4\n",
        "LEARNING_RATE = 5e-5\n",
        "MAX_SEQ_LENGTH = 512\n",
        "\n",
        "# 4-bit Quantization ì„¤ì • (ë©”ëª¨ë¦¬ ì ˆì•½)\n",
        "USE_4BIT = True  # T4 GPUì—ì„œëŠ” True ê¶Œì¥\n",
        "\n",
        "# ===============================================\n",
        "\n",
        "print(\"ğŸ“‹ Configuration:\")\n",
        "print(f\"   Model: {MODEL_NAME}\")\n",
        "print(f\"   Dataset: {DATASET_NAME} ({NUM_SAMPLES} samples)\")\n",
        "print(f\"   LoRA Rank: {LORA_R}\")\n",
        "print(f\"   Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"   Effective Batch Size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
        "print(f\"   4-bit Quantization: {USE_4BIT}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset"
      },
      "source": [
        "## 3ï¸âƒ£ ë°ì´í„°ì…‹ ì¤€ë¹„"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_dataset",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49114f69-ae55-4557-8b1d-a6a1970ee113"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“š Loading Korean dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Loaded nlpai-lab/kullm-v2\n",
            "ğŸ“Š Selected 3000 samples\n",
            "âœ… Formatted 2978 samples\n",
            "\n",
            "ğŸ“ Example data:\n",
            "============================================================\n",
            "<|user|>\n",
            "ì•„ë˜ íŒŒì´ì¬ í”„ë¡œê·¸ë¨ì—ì„œ ìŠ¤í¬ë©í•œ URLì´ ë‰´ìŠ¤ ê¸°ì‚¬ì¸ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ 5ê°œì˜ ê²€ì‚¬ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤. URLì— ë‰´ìŠ¤ ë˜ëŠ” ë³´ë„ ìë£Œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì—¬ í†µê³¼ëœ\\_urlì— ì¶”ê°€í•˜ê³ , ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ì‹¤íŒ¨í•œ\\_urlì— ì¶”ê°€í•©ë‹ˆë‹¤.CSS ì„ íƒê¸°ì— ë‰´ìŠ¤ ë˜ëŠ” ë³´ë„ ìë£Œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì—¬ í†µê³¼ ëœ \\_urlì— ì¶”ê°€í•˜ì§€ ì•Šìœ¼ë©´ ì‹¤íŒ¨í•œ \\_urlì— ì¶”ê°€í•©ë‹ˆë‹¤.ë§í¬ê°€ htmlì¸ì§€ í™•ì¸í•˜ì—¬ passed\\_urlì— ì¶”ê°€í•©ë‹ˆë‹¤.URLì´ pdf ë˜ëŠ” ë¯¸ë””ì–´ íŒŒì¼ í™•ì¥ì ë˜ëŠ” ì •ì  íŒŒì¼ í™•ì¥ìì¸ì§€ í™•ì¸í•œ í›„ failed\\_url...\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"ğŸ“š Loading Korean dataset...\")\n",
        "\n",
        "# í•œêµ­ì–´ instruction ë°ì´í„°ì…‹ ë¡œë“œ\n",
        "try:\n",
        "    dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
        "    print(f\"âœ… Loaded {DATASET_NAME}\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸  Failed to load {DATASET_NAME}, trying alternative...\")\n",
        "    # Note: The DATASET_NAME variable is already set in the hyperparameters cell.\n",
        "    # This fallback logic might be redundant if the hyperparameters cell is the single source of truth.\n",
        "    # Keeping it for now, but consider removing if it causes confusion.\n",
        "    dataset = load_dataset(\"nlpai-lab/kullm-v2\", split=\"train\") # Directly use the intended dataset name\n",
        "    print(f\"âœ… Loaded nlpai-lab/kullm-v2\")\n",
        "\n",
        "\n",
        "# ìƒ˜í”Œë§\n",
        "dataset = dataset.shuffle(seed=42).select(range(min(NUM_SAMPLES, len(dataset))))\n",
        "print(f\"ğŸ“Š Selected {len(dataset)} samples\")\n",
        "\n",
        "# ë°ì´í„° í¬ë§·íŒ…\n",
        "def format_prompt(examples):\n",
        "    \"\"\"ChatML í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ… (Batched version)\"\"\"\n",
        "    texts = []\n",
        "    for instruction, output in zip(examples.get(\"instruction\", []), examples.get(\"output\", [])):\n",
        "         # ë¹ˆ ë°ì´í„° í•„í„°ë§\n",
        "        if instruction and output:\n",
        "            text = f\"<|user|>\\n{instruction}</s>\\n<|assistant|>\\n{output}</s>\"\n",
        "            texts.append(text)\n",
        "        else:\n",
        "            texts.append(\"\") # ë¹ˆ ë¬¸ìì—´ ì¶”ê°€ ë˜ëŠ” í•´ë‹¹ ìƒ˜í”Œ ê±´ë„ˆë›°ê¸° (ì—¬ê¸°ì„œëŠ” ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬)\n",
        "\n",
        "    return {\"text\": texts}\n",
        "\n",
        "\n",
        "# í¬ë§· ì ìš© ë° í•„í„°ë§\n",
        "# Use batched=True for potentially faster processing\n",
        "# Remove columns after mapping\n",
        "dataset = dataset.map(format_prompt, remove_columns=dataset.column_names, batched=True)\n",
        "# Filter out empty strings resulting from invalid data\n",
        "dataset = dataset.filter(lambda x: len(x.get(\"text\", \"\")) > 10)\n",
        "\n",
        "\n",
        "print(f\"âœ… Formatted {len(dataset)} samples\")\n",
        "print(f\"\\nğŸ“ Example data:\")\n",
        "print(\"=\" * 60)\n",
        "# Ensure dataset is not empty before accessing elements\n",
        "if len(dataset) > 0:\n",
        "    print(dataset[0][\"text\"][:300] + \"...\")\n",
        "else:\n",
        "    print(\"No data left after filtering.\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model_loading"
      },
      "source": [
        "## 4ï¸âƒ£ ëª¨ë¸ ë¡œë”©"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d52b42b2",
        "outputId": "90a1ded5-6b29-44b0-cb91-841429d33c4f"
      },
      "source": [
        "print(\"â¬†ï¸ Upgrading bitsandbytes...\")\n",
        "!pip install -qU bitsandbytes\n",
        "print(\"âœ… bitsandbytes upgraded!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬†ï¸ Upgrading bitsandbytes...\n",
            "âœ… bitsandbytes upgraded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_model",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6974d7aa-ce90-43e3-d076-eef1591770bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¤– Loading model and tokenizer...\n",
            "   Using 4-bit quantization\n",
            "âœ… Model and tokenizer loaded\n",
            "   Total parameters: 1.10B\n"
          ]
        }
      ],
      "source": [
        "print(\"ğŸ¤– Loading model and tokenizer...\")\n",
        "\n",
        "# 4-bit quantization ì„¤ì • (ì˜µì…˜)\n",
        "if USE_4BIT:\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "    print(\"   Using 4-bit quantization\")\n",
        "else:\n",
        "    bnb_config = None\n",
        "    print(\"   Using full precision\")\n",
        "\n",
        "# ëª¨ë¸ ë¡œë“œ\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config if USE_4BIT else None,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16 if not USE_4BIT else None,\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "# 4-bit ëª¨ë¸ ì¤€ë¹„\n",
        "if USE_4BIT:\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "print(\"âœ… Model and tokenizer loaded\")\n",
        "print(f\"   Total parameters: {model.num_parameters() / 1e9:.2f}B\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lora_setup"
      },
      "source": [
        "## 5ï¸âƒ£ LoRA ì„¤ì •"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_lora",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceb14a38-a190-43a5-c706-50bff19bcbc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ Setting up LoRA...\n",
            "\n",
            "ğŸ“Š Trainable Parameters:\n",
            "trainable params: 4,505,600 || all params: 1,104,553,984 || trainable%: 0.4079\n",
            "\n",
            "ğŸ’¡ LoRAëŠ” ì „ì²´ ëª¨ë¸ì˜ 0.727%ë§Œ í•™ìŠµí•©ë‹ˆë‹¤!\n",
            "   ì „ì²´: 620,111,872 íŒŒë¼ë¯¸í„°\n",
            "   í•™ìŠµ: 4,505,600 íŒŒë¼ë¯¸í„°\n"
          ]
        }
      ],
      "source": [
        "print(\"ğŸ”§ Setting up LoRA...\")\n",
        "\n",
        "# LoRA ì„¤ì •\n",
        "lora_config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=TARGET_MODULES,\n",
        ")\n",
        "\n",
        "# LoRA ì ìš©\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# í•™ìŠµ íŒŒë¼ë¯¸í„° í™•ì¸\n",
        "print(\"\\nğŸ“Š Trainable Parameters:\")\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# ê³„ì‚°\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_percentage = 100 * trainable_params / total_params\n",
        "\n",
        "print(f\"\\nğŸ’¡ LoRAëŠ” ì „ì²´ ëª¨ë¸ì˜ {trainable_percentage:.3f}%ë§Œ í•™ìŠµí•©ë‹ˆë‹¤!\")\n",
        "print(f\"   ì „ì²´: {total_params:,} íŒŒë¼ë¯¸í„°\")\n",
        "print(f\"   í•™ìŠµ: {trainable_params:,} íŒŒë¼ë¯¸í„°\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test_before"
      },
      "source": [
        "## 6ï¸âƒ£ Fine-tuning ì „ í…ŒìŠ¤íŠ¸ (Before)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_original",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7dc07c5-4420-46c4-948c-c29452a15d9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§ª Testing ORIGINAL model (Before fine-tuning)\n",
            "\n",
            "======================================================================\n",
            "\n",
            "â“ Test 1: í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?\n",
            "ğŸ’¬ Answer: Korea's capital city is Seoul, located in the southeastern part of the country....\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "â“ Test 2: ê¹€ì¹˜ì°Œê°œ ë§Œë“œëŠ” ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”.\n",
            "ğŸ’¬ Answer: Kimchi is a traditional Korean dish made from fermented cabbage, garlic, and spices. Here's a simple recipe for making Kimchi at home:\n",
            "\n",
            "Ingredients:\n",
            "- 2 cups of fresh, finely chopped cabbage\n",
            "- 2 garli...\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "â“ Test 3: íŒŒì´ì¬ìœ¼ë¡œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ë ¬í•˜ëŠ” ë°©ë²•ì€?\n",
            "ğŸ’¬ Answer: Sure, here are a few methods for sorting a Python list in ascending or descending order:\n",
            "\n",
            "1. Using built-in sort function:\n",
            "\n",
            "```python\n",
            "# Example 1: Using built-in sort function\n",
            "sorted_list = [5, 3, 1, ...\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "ğŸ’¡ ëŒ€ë¶€ë¶„ ì˜ì–´ë¡œ ë‹µë³€í•˜ê±°ë‚˜ ì´ìƒí•œ ë‹µë³€ì„ í•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
            "   Fine-tuning í›„ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ ì§€ì¼œë´ì£¼ì„¸ìš”!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"ğŸ§ª Testing ORIGINAL model (Before fine-tuning)\\n\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸\n",
        "test_prompts = [\n",
        "    \"í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?\",\n",
        "    \"ê¹€ì¹˜ì°Œê°œ ë§Œë“œëŠ” ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”.\",\n",
        "    \"íŒŒì´ì¬ìœ¼ë¡œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ë ¬í•˜ëŠ” ë°©ë²•ì€?\",\n",
        "]\n",
        "\n",
        "# ì›ë³¸ ëª¨ë¸ ë¡œë“œ (LoRA ì—†ì´)\n",
        "original_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "original_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "if original_tokenizer.pad_token is None:\n",
        "    original_tokenizer.pad_token = original_tokenizer.eos_token\n",
        "\n",
        "# íŒŒì´í”„ë¼ì¸\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=original_model,\n",
        "    tokenizer=original_tokenizer,\n",
        "    max_new_tokens=150,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        ")\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸\n",
        "for i, prompt in enumerate(test_prompts, 1):\n",
        "    formatted_prompt = f\"<|user|>\\n{prompt}</s>\\n<|assistant|>\\n\"\n",
        "    result = pipe(formatted_prompt)[0][\"generated_text\"]\n",
        "\n",
        "    # ë‹µë³€ ì¶”ì¶œ\n",
        "    if \"<|assistant|>\" in result:\n",
        "        answer = result.split(\"<|assistant|>\")[-1].split(\"</s>\")[0].strip()\n",
        "    else:\n",
        "        answer = result[len(formatted_prompt):].strip()\n",
        "\n",
        "    print(f\"\\nâ“ Test {i}: {prompt}\")\n",
        "    print(f\"ğŸ’¬ Answer: {answer[:200]}...\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "# ë©”ëª¨ë¦¬ ì •ë¦¬\n",
        "del original_model, pipe\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\nğŸ’¡ ëŒ€ë¶€ë¶„ ì˜ì–´ë¡œ ë‹µë³€í•˜ê±°ë‚˜ ì´ìƒí•œ ë‹µë³€ì„ í•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
        "print(\"   Fine-tuning í›„ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ ì§€ì¼œë´ì£¼ì„¸ìš”!\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training"
      },
      "source": [
        "## 7ï¸âƒ£ LoRA Fine-tuning ğŸš€\n",
        "\n",
        "ì´ì œ ë³¸ê²©ì ìœ¼ë¡œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤! (ì•½ 15-20ë¶„ ì†Œìš”)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train_model",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5f4398a4-f234-4fd5-edc2-cac0b3cfcacf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Starting LoRA Fine-tuning...\n",
            "\n",
            "â° Training started at: 16:08:55\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n",
            "\n",
            "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ğŸ¤— Transformers. Use `--hub_token` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:477: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“Š Total steps: 2235\n",
            "\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='558' max='558' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [558/558 36:44, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.189700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.161100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.145700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.199500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.221800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.099700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.109800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.079700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.110000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.086700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.079300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.075200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>1.083300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.137400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.097400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>1.088100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>1.075200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>1.127700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>1.070800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.065200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>1.069500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>1.068400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>1.100300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>1.067100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.018800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>1.016600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>1.032400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>1.036900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>1.025700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.049600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>1.052900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>1.041000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>1.035000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>1.044800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.072900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>1.045300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>1.005700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>1.006300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>1.015800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.064000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>1.046200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>1.007800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>1.055500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>1.037600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>1.031700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>1.050100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>1.009000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>1.015900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>1.061600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.022500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>1.002300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>1.029000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>1.036900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>1.050200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>1.043600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "âœ… Training completed!\n",
            "â° Duration: 0:36:52.880041\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"ğŸš€ Starting LoRA Fine-tuning...\\n\")\n",
        "\n",
        "# í•™ìŠµ ì„¤ì •\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    fp16=True,\n",
        "    logging_steps=10,\n",
        "    save_steps=200,\n",
        "    save_total_limit=2,\n",
        "    warmup_steps=50,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    optim=\"paged_adamw_32bit\" if USE_4BIT else \"adamw_torch\",\n",
        "    gradient_checkpointing=True,\n",
        "    report_to=\"none\",\n",
        "    group_by_length=True,\n",
        ")\n",
        "\n",
        "# Trainer ì„¤ì •\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    tokenizer=tokenizer,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    peft_config=lora_config,\n",
        ")\n",
        "\n",
        "# í•™ìŠµ ì‹œì‘ ì‹œê°„ ê¸°ë¡\n",
        "start_time = datetime.now()\n",
        "print(f\"â° Training started at: {start_time.strftime('%H:%M:%S')}\")\n",
        "print(f\"ğŸ“Š Total steps: {len(trainer.get_train_dataloader()) * NUM_EPOCHS}\")\n",
        "print(f\"\\n{'='*70}\\n\")\n",
        "\n",
        "# í•™ìŠµ ì‹¤í–‰!\n",
        "trainer.train()\n",
        "\n",
        "# í•™ìŠµ ì¢…ë£Œ ì‹œê°„\n",
        "end_time = datetime.now()\n",
        "duration = end_time - start_time\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"âœ… Training completed!\")\n",
        "print(f\"â° Duration: {duration}\")\n",
        "print(f\"{'='*70}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save_model",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efbb96d0-0df3-408f-ef04-8696cb42c4c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saving LoRA adapter to ./korean-tinyllama-lora...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Saved!\n",
            "\n",
            "ğŸ“ Saved files:\n",
            "total 20M\n",
            "-rw-r--r-- 1 root root  688 Oct 16 16:45 adapter_config.json\n",
            "-rw-r--r-- 1 root root  18M Oct 16 16:45 adapter_model.safetensors\n",
            "-rw-r--r-- 1 root root 5.0K Oct 16 16:45 README.md\n",
            "-rw-r--r-- 1 root root  551 Oct 16 16:45 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root 1.4K Oct 16 16:45 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root 1.8M Oct 16 16:45 tokenizer.json\n",
            "-rw-r--r-- 1 root root 489K Oct 16 16:45 tokenizer.model\n"
          ]
        }
      ],
      "source": [
        "# LoRA adapter ì €ì¥\n",
        "print(f\"ğŸ’¾ Saving LoRA adapter to {OUTPUT_DIR}...\")\n",
        "\n",
        "trainer.model.save_pretrained(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "print(f\"âœ… Saved!\")\n",
        "print(f\"\\nğŸ“ Saved files:\")\n",
        "!ls -lh {OUTPUT_DIR}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test_after"
      },
      "source": [
        "## 8ï¸âƒ£ Fine-tuning í›„ í…ŒìŠ¤íŠ¸ (After)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_finetuned",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7a0f836-a11c-4b94-d551-e36f9a7321e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¤– Loading fine-tuned model...\n",
            "\n",
            "âœ… Model loaded!\n"
          ]
        }
      ],
      "source": [
        "print(\"ğŸ¤– Loading fine-tuned model...\\n\")\n",
        "\n",
        "# Fine-tuned ëª¨ë¸ ë¡œë“œ\n",
        "finetuned_model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "    OUTPUT_DIR,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "finetuned_model = finetuned_model.merge_and_unload()\n",
        "\n",
        "finetuned_tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR)\n",
        "if finetuned_tokenizer.pad_token is None:\n",
        "    finetuned_tokenizer.pad_token = finetuned_tokenizer.eos_token\n",
        "\n",
        "print(\"âœ… Model loaded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_finetuned",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a18b9e78-3580-430c-ba1b-c870ee24269a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§ª Testing FINE-TUNED model (After fine-tuning)\n",
            "\n",
            "======================================================================\n",
            "\n",
            "â“ Test 1: í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?\n",
            "ğŸ’¬ Answer:\n",
            "í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸, ì¶œì¥ìƒµ, ë…¸ì›, í™ì½© ë“±ì…ë‹ˆë‹¤. ìµœê·¼ ë°œì „ëœ ìˆ˜ë„ëŠ” ë…¸ì›ì´ ë˜ì—ˆìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  ë¶ìª½ì— ìˆëŠ” í™ì½©ë„ ë” ë¹ ë¥´ê²Œ ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤. í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì•½ 60ê°œì…ë‹ˆë‹¤. ë³¸ë¬¸ì—ì„œ ì•Œì•„ë³¸ ìˆ˜ë„ëŠ” í™ì½©, ì¶œì¥ìƒµ, ï¿½ï¿½\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "â“ Test 2: ê¹€ì¹˜ì°Œê°œ ë§Œë“œëŠ” ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”.\n",
            "ğŸ’¬ Answer:\n",
            "ê¹€ì¹˜ì°Œê°œë¥¼ ë§Œë“œëŠ” ë²•ì„ ì„¤ëª…í•˜ëŠ” ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
            "\n",
            "1. ì˜ë ¤ëœ ì¹´í˜ ë² ë””ë“œ í”Œë¼ìŠ¤í‹± ë¸”ë ˆì´ë“œì— ì•Œë§ì€ ì¹´í˜ ë¸”ë ˆì´ë“œ ë°°ë‹¬ ë§¤ì²´ë¥¼ ë„£ìŠµë‹ˆë‹¤.\n",
            "\n",
            "2. í¬ë˜ìŠ¤í„° ë˜ëŠ” ì¹´í˜ ë¸”ë ˆì´ë“œ ë°°ë‹¬ ë§¤ì²´ì— ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "â“ Test 3: íŒŒì´ì¬ìœ¼ë¡œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ë ¬í•˜ëŠ” ë°©ë²•ì€?\n",
            "ğŸ’¬ Answer:\n",
            "ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ë ¬í•˜ëŠ” ë°©ë²•ì—ëŠ” ì—¬ëŸ¬ ê°€ì§€ ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒì€ íŒŒì´ì¬ 3.8 ë²„ì „ì—ì„œ ê°€ì¥ ì¼ë°˜ì ì¸ ë°©ë²•ì…ë‹ˆë‹¤:\n",
            "\n",
            "```python\n",
            "# ì›ë³¸ ë¦¬ìŠ¤íŠ¸ì—ì„œ ìš”ì†Œ 0ë¶€í„° 25ê¹Œì§€ ìˆœíšŒí•˜ê³  ê·¸ ê°’ì„ ë¦¬ìŠ¤íŠ¸ì˜ ë°°ì—´ì— ì¶”ê°€í•©ë‹ˆë‹¤.\n",
            "arr = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
            "arr.sort()\n",
            "print(arr)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "âœ¨ ì´ì œ í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\n"
          ]
        }
      ],
      "source": [
        "print(\"ğŸ§ª Testing FINE-TUNED model (After fine-tuning)\\n\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ë™ì¼í•œ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸\n",
        "test_prompts = [\n",
        "    \"í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?\",\n",
        "    \"ê¹€ì¹˜ì°Œê°œ ë§Œë“œëŠ” ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”.\",\n",
        "    \"íŒŒì´ì¬ìœ¼ë¡œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ë ¬í•˜ëŠ” ë°©ë²•ì€?\",\n",
        "]\n",
        "\n",
        "# íŒŒì´í”„ë¼ì¸\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=finetuned_model,\n",
        "    tokenizer=finetuned_tokenizer,\n",
        "    max_new_tokens=200,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        ")\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸\n",
        "for i, prompt in enumerate(test_prompts, 1):\n",
        "    formatted_prompt = f\"<|user|>\\n{prompt}</s>\\n<|assistant|>\\n\"\n",
        "    result = pipe(formatted_prompt)[0][\"generated_text\"]\n",
        "\n",
        "    # ë‹µë³€ ì¶”ì¶œ\n",
        "    if \"<|assistant|>\" in result:\n",
        "        answer = result.split(\"<|assistant|>\")[-1].split(\"</s>\")[0].strip()\n",
        "    else:\n",
        "        answer = result[len(formatted_prompt):].strip()\n",
        "\n",
        "    print(f\"\\nâ“ Test {i}: {prompt}\")\n",
        "    print(f\"ğŸ’¬ Answer:\\n{answer}\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "print(\"\\nâœ¨ ì´ì œ í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "539a5c5b",
        "outputId": "2363dc18-c3a9-47b3-c040-1bfb36b985be"
      },
      "source": [
        "print(\"ğŸ§ª Original vs Fine-tuned Model Comparison\\n\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ì›ë³¸ ëª¨ë¸ ë¡œë“œ (ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆë‹¤ë©´ ìƒëµ ê°€ëŠ¥)\n",
        "try:\n",
        "    original_model\n",
        "except NameError:\n",
        "    print(\"ğŸ¤– Loading original model and tokenizer for comparison...\")\n",
        "    original_model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16,\n",
        "    )\n",
        "    original_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    if original_tokenizer.pad_token is None:\n",
        "        original_tokenizer.pad_token = original_tokenizer.eos_token\n",
        "    print(\"âœ… Original model loaded!\")\n",
        "\n",
        "\n",
        "# ì›ë³¸ ëª¨ë¸ íŒŒì´í”„ë¼ì¸\n",
        "original_pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=original_model,\n",
        "    tokenizer=original_tokenizer,\n",
        "    max_new_tokens=150, # ì›ë³¸ í…ŒìŠ¤íŠ¸ ì…€ê³¼ ë™ì¼í•˜ê²Œ ì„¤ì •\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        ")\n",
        "\n",
        "# Fine-tuned ëª¨ë¸ íŒŒì´í”„ë¼ì¸ (ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆë‹¤ë©´ ì¬ì‚¬ìš©)\n",
        "# 'pipe' ë³€ìˆ˜ëŠ” ì´ì „ ì…€ì—ì„œ ì •ì˜ëœ finetuned_model íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤.\n",
        "try:\n",
        "    pipe # ì´ì „ ì…€ì—ì„œ ì •ì˜ëœ finetuned_model íŒŒì´í”„ë¼ì¸ ì‚¬ìš©\n",
        "except NameError:\n",
        "    print(\"ğŸ¤– Loading fine-tuned model for comparison...\")\n",
        "    finetuned_model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "        OUTPUT_DIR,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16,\n",
        "    )\n",
        "    finetuned_model = finetuned_model.merge_and_unload()\n",
        "    finetuned_tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR)\n",
        "    if finetuned_tokenizer.pad_token is None:\n",
        "        finetuned_tokenizer.pad_token = finetuned_tokenizer.eos_token\n",
        "\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=finetuned_model,\n",
        "        tokenizer=finetuned_tokenizer,\n",
        "        max_new_tokens=200, # fine-tuned í…ŒìŠ¤íŠ¸ ì…€ê³¼ ë™ì¼í•˜ê²Œ ì„¤ì •\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "    print(\"âœ… Fine-tuned model loaded!\")\n",
        "\n",
        "\n",
        "# ë¹„êµí•  ì§ˆë¬¸ (ì´ì „ ì…€ì—ì„œ ì •ì˜ëœ your_question ë³€ìˆ˜ ì‚¬ìš©)\n",
        "try:\n",
        "    your_question\n",
        "except NameError:\n",
        "    your_question = \"ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?\" # your_questionì´ ì •ì˜ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©\n",
        "    print(f\"âš ï¸ 'your_question'ì´ ì •ì˜ë˜ì§€ ì•Šì•„ ê¸°ë³¸ ì§ˆë¬¸ '{your_question}'ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
        "\n",
        "\n",
        "formatted_prompt = f\"<|user|>\\n{your_question}</s>\\n<|assistant|>\\n\"\n",
        "\n",
        "print(f\"â“ Your Question: {your_question}\\n\")\n",
        "\n",
        "# ì›ë³¸ ëª¨ë¸ ì‘ë‹µ\n",
        "print(\"=== Original Model Response ===\")\n",
        "original_result = original_pipe(formatted_prompt)[0][\"generated_text\"]\n",
        "if \"<|assistant|>\" in original_result:\n",
        "    original_answer = original_result.split(\"<|assistant|>\")[-1].split(\"</s>\")[0].strip()\n",
        "else:\n",
        "    original_answer = original_result[len(formatted_prompt):].strip()\n",
        "print(original_answer)\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Fine-tuned ëª¨ë¸ ì‘ë‹µ\n",
        "print(\"=== Fine-tuned Model Response ===\")\n",
        "finetuned_result = pipe(formatted_prompt)[0][\"generated_text\"]\n",
        "if \"<|assistant|>\" in finetuned_result:\n",
        "    finetuned_answer = finetuned_result.split(\"<|assistant|>\")[-1].split(\"</s>\")[0].strip()\n",
        "else:\n",
        "    finetuned_answer = finetuned_result[len(formatted_prompt):].strip()\n",
        "print(finetuned_answer)\n",
        "print(\"-\" * 30)\n",
        "\n",
        "print(\"\\nâœ¨ ë‘ ëª¨ë¸ì˜ ë‹µë³€ ì°¨ì´ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”!\")\n",
        "\n",
        "# ë©”ëª¨ë¦¬ ì •ë¦¬ (ì„ íƒì‚¬í•­)\n",
        "# del original_model, original_pipe\n",
        "# torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§ª Original vs Fine-tuned Model Comparison\n",
            "\n",
            "======================================================================\n",
            "ğŸ¤– Loading original model and tokenizer for comparison...\n",
            "âœ… Original model loaded!\n",
            "âš ï¸ 'your_question'ì´ ì •ì˜ë˜ì§€ ì•Šì•„ ê¸°ë³¸ ì§ˆë¬¸ 'ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?'ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
            "â“ Your Question: ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?\n",
            "\n",
            "=== Original Model Response ===\n",
            "ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ëŒ€í•œë¯¼êµ­ ì „ì²´ ì‚¬ì´íŠ¸ì— ìˆìŠµë‹ˆë‹¤. ëŒ€í•œë¯¼êµ­ì— ìˆëŠ” ëª¨ë“  ë„ì‹œ, êµ°ì‚¬ ê¸°ê´€, ê³µê³µ ê¸°ê´€, ì •ë¶€ ì„¤ë¦½ì²´ê³„ ë“±ì´ ëª¨ë‘ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
            "------------------------------\n",
            "=== Fine-tuned Model Response ===\n",
            "ëŒ€í•œë¯¼êµ­ì˜ ì£¼ìš” ìˆ˜ë„ëŠ” í™ì½©, ë‰´ìš•, ìº˜ë¦¬í¬ë‹ˆì•„, ë² ë¥´ë² ì´ë‹ˆì¹˜, ë£¨ë…¸ë¦¬ì•„, ì—˜ë¦¬ë² ì´í„°, ë§ë ˆì¸ë°”, í¬ë¥´í† ë„¤, ì¹´íƒˆëœë“œ, í¬ë˜ë””ì–¸, ì¹´ë¥´í† í¬ë¦­, ë¸Œë¼ì§ˆ, ë² ì´ë¹„ ì½”ë¦¬ì•„, ì•„ë©”ë¦¬ì¹¸, ë„¤ëœë€ë“œ, ë² ë„¤ìˆ˜ì—˜ë¼,\n",
            "------------------------------\n",
            "\n",
            "âœ¨ ë‘ ëª¨ë¸ì˜ ë‹µë³€ ì°¨ì´ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "interactive"
      },
      "source": [
        "## 9ï¸âƒ£ ì§ì ‘ í…ŒìŠ¤íŠ¸í•´ë³´ê¸°!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "interactive_test",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0523ca8-ef88-4e50-d9db-9de617a8bb5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â“ Your Question: ëŒ€í•œë¯¼êµ­ì˜ ì „í†µ ìŒì‹ì—ëŠ” ë¬´ì—‡ì´ ìˆë‚˜ìš”?\n",
            "\n",
            "======================================================================\n",
            "ğŸ’¬ Answer:\n",
            "ëŒ€í•œë¯¼êµ­ì˜ ì „í†µ ìŒì‹ì—ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì—¬ëŸ¬ ì¢…ë¥˜ê°€ ìˆìŠµë‹ˆë‹¤:\n",
            "\n",
            "1. ë†ì—…ì‹: ë†ì—…ì‹ì€ ë†ì—… ì‚°ë¬¼ì„ í†µí•´ ìƒì‚°ë˜ëŠ” ì§ˆì†Œ ì†Œì§„ì˜ ì´ˆë°˜ ê²°ê³¼ë¬¼ì…ë‹ˆë‹¤. ë†ì—…ì‹ì€ ë‹¹ì—°íˆ ë†ì—…ì— ë”°ë¼ ë‹¬ë¼ì§‘ë‹ˆë‹¤.\n",
            "\n",
            "2. ì¹˜ë§ˆ: ì¹˜ë§ˆëŠ” ë†ì—…ì— ë”°ë¼ ë‹¬ë¼ï¿½\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ì›í•˜ëŠ” ì§ˆë¬¸ì„ ì…ë ¥í•´ë³´ì„¸ìš”!\n",
        "your_question = \"ëŒ€í•œë¯¼êµ­ì˜ ì „í†µ ìŒì‹ì—ëŠ” ë¬´ì—‡ì´ ìˆë‚˜ìš”?\"  # ì—¬ê¸°ë¥¼ ìˆ˜ì •í•˜ì„¸ìš”\n",
        "\n",
        "print(f\"â“ Your Question: {your_question}\\n\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "formatted_prompt = f\"<|user|>\\n{your_question}</s>\\n<|assistant|>\\n\"\n",
        "result = pipe(formatted_prompt)[0][\"generated_text\"]\n",
        "\n",
        "# ë‹µë³€ ì¶”ì¶œ\n",
        "if \"<|assistant|>\" in result:\n",
        "    answer = result.split(\"<|assistant|>\")[-1].split(\"</s>\")[0].strip()\n",
        "else:\n",
        "    answer = result[len(formatted_prompt):].strip()\n",
        "\n",
        "print(f\"ğŸ’¬ Answer:\\n{answer}\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download"
      },
      "source": [
        "## ğŸ”Ÿ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ì„ íƒì‚¬í•­)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zip_model",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c5bf33c-a19d-4b78-e388-6c0ba3c8dcb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: korean-tinyllama-lora/ (stored 0%)\n",
            "updating: korean-tinyllama-lora/adapter_model.safetensors (deflated 8%)\n",
            "updating: korean-tinyllama-lora/README.md (deflated 66%)\n",
            "updating: korean-tinyllama-lora/tokenizer.json (deflated 74%)\n",
            "updating: korean-tinyllama-lora/tokenizer_config.json (deflated 69%)\n",
            "updating: korean-tinyllama-lora/special_tokens_map.json (deflated 79%)\n",
            "updating: korean-tinyllama-lora/tokenizer.model (deflated 55%)\n",
            "updating: korean-tinyllama-lora/adapter_config.json (deflated 52%)\n",
            "âœ… Zipped!\n",
            "ğŸ“¥ ë‹¤ìš´ë¡œë“œ: ì¢Œì¸¡ íŒŒì¼ íƒ­ì—ì„œ 'korean-tinyllama-lora.zip'ì„ ì°¾ì•„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.\n",
            "\n",
            "ğŸ’¡ ë¡œì»¬ì—ì„œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•:\n",
            "\n",
            "from peft import AutoPeftModelForCausalLM\n",
            "from transformers import AutoTokenizer\n",
            "\n",
            "# LoRA adapter ë¡œë“œ\n",
            "model = AutoPeftModelForCausalLM.from_pretrained(\"./korean-tinyllama-lora\")\n",
            "model = model.merge_and_unload()\n",
            "tokenizer = AutoTokenizer.from_pretrained(\"./korean-tinyllama-lora\")\n",
            "\n",
            "# ì¶”ë¡ \n",
            "prompt = \"<|user|>\\nì•ˆë…•í•˜ì„¸ìš”!</s>\\n<|assistant|>\\n\"\n",
            "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
            "outputs = model.generate(**inputs, max_new_tokens=100)\n",
            "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# LoRA adapterë¥¼ zipìœ¼ë¡œ ì••ì¶•í•˜ì—¬ ë‹¤ìš´ë¡œë“œ\n",
        "!zip -r korean-tinyllama-lora.zip {OUTPUT_DIR}\n",
        "\n",
        "print(\"âœ… Zipped!\")\n",
        "print(\"ğŸ“¥ ë‹¤ìš´ë¡œë“œ: ì¢Œì¸¡ íŒŒì¼ íƒ­ì—ì„œ 'korean-tinyllama-lora.zip'ì„ ì°¾ì•„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.\")\n",
        "print(\"\\nğŸ’¡ ë¡œì»¬ì—ì„œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•:\")\n",
        "print(\"\"\"\\nfrom peft import AutoPeftModelForCausalLM\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# LoRA adapter ë¡œë“œ\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(\"./korean-tinyllama-lora\")\n",
        "model = model.merge_and_unload()\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./korean-tinyllama-lora\")\n",
        "\n",
        "# ì¶”ë¡ \n",
        "prompt = \"<|user|>\\\\nì•ˆë…•í•˜ì„¸ìš”!</s>\\\\n<|assistant|>\\\\n\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=100)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdrive"
      },
      "source": [
        "## 1ï¸âƒ£1ï¸âƒ£ Google Driveì— ì €ì¥ (ì„ íƒì‚¬í•­)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mount_drive",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c2aa290-8a3a-4963-b352-6602bbb42b85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "âœ… Saved to Google Drive!\n",
            "ğŸ“ Location: MyDrive/korean-tinyllama-lora\n"
          ]
        }
      ],
      "source": [
        "# Google Drive ë§ˆìš´íŠ¸\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Driveì— ë³µì‚¬\n",
        "!cp -r {OUTPUT_DIR} /content/drive/MyDrive/korean-tinyllama-lora\n",
        "\n",
        "print(\"âœ… Saved to Google Drive!\")\n",
        "print(\"ğŸ“ Location: MyDrive/korean-tinyllama-lora\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary"
      },
      "source": [
        "## ğŸ“Š ìš”ì•½ ë° ë‹¤ìŒ ë‹¨ê³„\n",
        "\n",
        "### âœ… ì™„ë£Œí•œ ì‘ì—…\n",
        "1. âœ… TinyLlama ëª¨ë¸ ë¡œë“œ\n",
        "2. âœ… í•œêµ­ì–´ ë°ì´í„°ì…‹ ì¤€ë¹„\n",
        "3. âœ… LoRA ì„¤ì • ë° ì ìš©\n",
        "4. âœ… Fine-tuning ì‹¤í–‰\n",
        "5. âœ… Before/After ë¹„êµ í…ŒìŠ¤íŠ¸\n",
        "\n",
        "### ğŸ¯ ì£¼ìš” ê²°ê³¼\n",
        "- **ì „ì²´ íŒŒë¼ë¯¸í„°**: 1.1B\n",
        "- **í•™ìŠµ íŒŒë¼ë¯¸í„°**: ~5-10M (ì•½ 0.5%ë§Œ í•™ìŠµ!)\n",
        "- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: 4-bit ì‚¬ìš© ì‹œ ~6GB\n",
        "- **í•™ìŠµ ì‹œê°„**: T4 GPU ê¸°ì¤€ ~15-20ë¶„\n",
        "\n",
        "### ğŸš€ ë‹¤ìŒ ë‹¨ê³„\n",
        "1. **ë” ë§ì€ ë°ì´í„°ë¡œ í•™ìŠµ**: `NUM_SAMPLES`ë¥¼ ëŠ˜ë ¤ë³´ì„¸ìš”\n",
        "2. **í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹**: `LORA_R`, `LEARNING_RATE` ì¡°ì •\n",
        "3. **ë‹¤ë¥¸ ëª¨ë¸ ì‹œë„**: Llama-2, Mistral ë“±\n",
        "4. **DPO (Direct Preference Optimization)**: ì„ í˜¸ë„ ê¸°ë°˜ ì¶”ê°€ í•™ìŠµ\n",
        "5. **ë°°í¬**: HuggingFace Hubì— ì—…ë¡œë“œí•˜ì—¬ ê³µìœ \n",
        "\n",
        "### ğŸ“š ì°¸ê³  ìë£Œ\n",
        "- [LoRA ë…¼ë¬¸](https://arxiv.org/abs/2106.09685)\n",
        "- [PEFT ë¼ì´ë¸ŒëŸ¬ë¦¬](https://github.com/huggingface/peft)\n",
        "- [TRL ë¼ì´ë¸ŒëŸ¬ë¦¬](https://github.com/huggingface/trl)\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ‰ ì¶•í•˜í•©ë‹ˆë‹¤!\n",
        "LoRAë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ì–´ ëª¨ë¸ì„ í•œêµ­ì–´ë¡œ ì„±ê³µì ìœ¼ë¡œ ì ì‘ì‹œì¼°ìŠµë‹ˆë‹¤!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}